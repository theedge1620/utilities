{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operating Experience Data Daily Update\n",
    "\n",
    "This file performs the daily data updates for IOEB boards.  This includes the following:\n",
    "\n",
    "1. COVID-19 Local Conditions Update from JHU CSSE on GitHub\n",
    "2. Power Status Update from data warehouse and push to Box for INPO\n",
    "3. Findings Data Update from data warehouse\n",
    "4. Event Notification data update from data warehouse\n",
    "\n",
    "Items to be added: \n",
    "\n",
    "1. Power Status push to Box for INPO\n",
    "2. OpE Documents update from ADAMS public WBA and internal APIs\n",
    "\n",
    "Requirements:\n",
    "\n",
    "Python packages:\n",
    "    pandas\n",
    "    numpy\n",
    "    datetime\n",
    "    selenium\n",
    "    os\n",
    "    re\n",
    "    shutil\n",
    "    pyodbc\n",
    "    time\n",
    "    \n",
    "Sharepoint folders synced to user's U.S. NRC OneDrive:\n",
    "    \n",
    "https://usnrc.sharepoint.com/teams/COVID-19SiteImpacts/Shared%20Documents/Forms/AllItems.aspx\n",
    "    \n",
    "https://usnrc.sharepoint.com/teams/ROPDashboards/Shared%20Documents/Forms/AllItems.aspx\n",
    "    \n",
    "    \n",
    "Note:  to use selenium, you'll need to download chromedriver from:\n",
    "  \n",
    "https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "      \n",
    "and copy it to the correct location:\n",
    "    \n",
    "C:\\ProgramData\\ChromeDriver\\\n",
    "\n",
    "Note:\n",
    "\n",
    "If pyodbc fails to update from the data warehouse, attempt to connect to data warehouse via Excel prior to running code to establish connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary python packages:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import pyodbc\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import xml.etree.ElementTree as ET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set troubleshooting mode - this will display headers and other information if the user is troubleshooting\n",
    "# Set to zero to suppress unecessary output for normal runs, set to 1 to view all output.\n",
    "trouble_flag = 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID-19 Data Update for Local Conditions (Site Status) Board\n",
    "\n",
    "This portion of the script updates the source files from the COVID-19 Site Status Board.  It pulls the current data file from the JHU CSSE GitHub page at:\n",
    "\n",
    "https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv\n",
    "\n",
    "and performs calculations to determine the local condition surround NRC facilities, based on the EPZ counties surrounding each operating reactor and a list of counties of interest for other facilities.  The county list and other files are hosted on a Sharepoint Teams site at:\n",
    "\n",
    "https://usnrc.sharepoint.com/teams/COVID-19SiteImpacts/Shared%20Documents/Forms/AllItems.aspx\n",
    "\n",
    "This code assumes that the user has synced this folder to their U.S. NRC OneDrive\n",
    "\n",
    "Author:  Rebecca Sigmon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download current COVID-19 data from JHU CSSE:\n",
    "url=\"https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv\"\n",
    "time_series = pd.read_csv(url)\n",
    "\n",
    "# (Optional) - display header for the downloaded file\n",
    "if trouble_flag == 1:\n",
    "    time_series.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a combined_key to differentiate between counties with the same name in different states\n",
    "time_series[\"Combined_Key\"] = time_series[\"Admin2\"]+\", \"+time_series[\"Province_State\"]+\", \"+time_series[\"iso2\"]\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    time_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab the counties of interest from epz_counties spreadsheet\n",
    "sites_old = pd.read_excel(os.path.expanduser(\"~\") + '\\\\U.S. NRC\\\\COVID-19 Site Impacts - Documents\\\\site_status\\\\epz_counties.xlsx')\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    sites_old.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab the US population by county spreadsheet\n",
    "pop = pd.read_excel(os.path.expanduser(\"~\") + '\\\\U.S. NRC\\\\COVID-19 Site Impacts - Documents\\\\site_status\\\\US_pop.xlsx')\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    pop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join the site spreadsheet with the population spreadsheet to add a population column to the site spreadsheet\n",
    "sites = pop.merge(sites_old, left_on = \"Key\", right_on = \"Combined_Key\", how = \"inner\")\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    sites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a dataframe with just the case data for the counties of interest\n",
    "time_series_sites = time_series.merge(sites, left_on = \"Combined_Key\", right_on = \"Combined_Key\", how = \"inner\")\n",
    "time_series_sites[\"County\"] = time_series_sites[\"County_x\"]\n",
    "time_series_sites[\"State\"] = time_series_sites[\"State_x\"]\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    time_series_sites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up a dataframe with each county-site pair\n",
    "counties_sites = time_series_sites[[\"FIPS\", \"Site\", \"Region\", \"County\",\"State\",\"Combined_Key\", \"Population\"]]\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    counties_sites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the timeframe of interest in a way that doesn't have to be updated each day\n",
    "\n",
    "startdate = datetime.datetime(2020,3,4)\n",
    "today = datetime.datetime.today()\n",
    "offset = 53 + (today-startdate).days\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract just the confirmed case numbers by date\n",
    "dates = time_series_sites.iloc[:,53:offset]\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create time series dataframe:\n",
    "\n",
    "county_time_series = pd.DataFrame(columns = [\"FIPS\",\"Site\",\"Region\",\"County\",\"State\",\"Combined_Key\",\"Confirmed\",\"Date\",])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JBC4\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6201: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "#take the data from the columns and make each county-date entry a new row\n",
    "cols = dates.columns\n",
    "\n",
    "for i in range(0,len(cols)):\n",
    "    one_day = pd.DataFrame(columns = [\"Confirmed\", \"Date\"])\n",
    "    one_day[\"Confirmed\"] = dates.iloc[:,i]\n",
    "    one_day[\"Date\"] = cols[i]\n",
    "    county_time = counties_sites.join(one_day, how = \"left\", sort = False)\n",
    "    county_time_series = county_time_series.append(county_time)\n",
    "#    county_time.append(one_day)\n",
    "#    print(one_day)\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    county_time_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset time series index:\n",
    "\n",
    "county_time_series.reset_index(inplace = True)\n",
    "county_time_series.drop(\"index\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert date column to date-time object to allow future data manipulations\n",
    "county_time_series[\"Date\"] = pd.to_datetime(county_time_series.Date)\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    county_time_series.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a unique key other than the index to allow better defining relationships in powerBI\n",
    "county_time_series[\"Unique\"] = county_time_series[\"Site\"] + \", \"+ county_time_series[\"County\"]+\", \"+county_time_series[\"Date\"].astype(str)\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    county_time_series.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by unique column then by date\n",
    "county_time_series.sort_values([\"Unique\", \"Date\"], inplace = True)\n",
    "county_time_series.reset_index(inplace = True)\n",
    "county_time_series.drop(\"index\", axis = 1, inplace = True)\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    county_time_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change names because of code merge\n",
    "rolling_avg = county_time_series\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    rolling_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate new cases each day by subracting previous day's confirmed cases \n",
    "rolling_avg[\"New_Cases\"] = rolling_avg[\"Confirmed\"]-rolling_avg[\"Confirmed\"].shift(1)\n",
    "rolling_avg = rolling_avg[rolling_avg[\"Date\"] != \"2020-03-04\"]\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    rolling_avg.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JBC4\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#calculate the 14-day rolling average of confirmed cases using each of the previous 14 days' worth of data\n",
    "rolling_avg[\"Rolling_Avg\"] = (rolling_avg[\"Confirmed\"]+rolling_avg[\"Confirmed\"].shift(1)+rolling_avg[\"Confirmed\"].shift(2)+rolling_avg[\"Confirmed\"].shift(3)+rolling_avg[\"Confirmed\"].shift(4)+rolling_avg[\"Confirmed\"].shift(5)+rolling_avg[\"Confirmed\"].shift(6)+rolling_avg[\"Confirmed\"].shift(7)+rolling_avg[\"Confirmed\"].shift(8)+rolling_avg[\"Confirmed\"].shift(9)+rolling_avg[\"Confirmed\"].shift(10)+rolling_avg[\"Confirmed\"].shift(11)+rolling_avg[\"Confirmed\"].shift(12)+rolling_avg[\"Confirmed\"].shift(13))/14\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    rolling_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JBC4\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#calculate a 14-day rolling average of the number of new cases each day\n",
    "rolling_avg[\"New_Case_Roll_Avg\"] = (rolling_avg[\"New_Cases\"]+rolling_avg[\"New_Cases\"].shift(1)+rolling_avg[\"New_Cases\"].shift(2)+rolling_avg[\"New_Cases\"].shift(3)+rolling_avg[\"New_Cases\"].shift(4)+rolling_avg[\"New_Cases\"].shift(5)+rolling_avg[\"New_Cases\"].shift(6)+rolling_avg[\"New_Cases\"].shift(7)+rolling_avg[\"New_Cases\"].shift(8)+rolling_avg[\"New_Cases\"].shift(9)+rolling_avg[\"New_Cases\"].shift(10)+rolling_avg[\"New_Cases\"].shift(11)+rolling_avg[\"New_Cases\"].shift(12)+rolling_avg[\"New_Cases\"].shift(13))/14\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    rolling_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trouble_flag == 1:\n",
    "    rolling_avg.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JBC4\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#calculate the change(slope) of the 14-day rolling average of new cases over assigned timeframe\n",
    "timeframe = 14\n",
    "rolling_avg[\"Change_New_Case_Avg\"] = (rolling_avg[\"New_Case_Roll_Avg\"] - rolling_avg[\"New_Case_Roll_Avg\"].shift(timeframe-1))/timeframe\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    rolling_avg.tail(timeframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = datetime.timedelta(days = 14)\n",
    "firstdate = startdate+offset\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    firstdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_avg = rolling_avg[rolling_avg[\"Date\"] > firstdate]\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    rolling_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the change in the 14-day rolling average of new cases by dividing by the rolling avg of new cases\n",
    "rolling_avg[\"Normalized_Change\"] = rolling_avg.apply(lambda row: row.Change_New_Case_Avg/row.New_Case_Roll_Avg if row.New_Case_Roll_Avg != 0 else 0, axis = 1)\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    rolling_avg.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_avg[[\"Confirmed\",\"Population\",\"New_Cases\",\"Rolling_Avg\",\"New_Case_Roll_Avg\",\"Change_New_Case_Avg\",\n",
    "             \"Normalized_Change\"]] = rolling_avg[[\"Confirmed\",\"Population\",\"New_Cases\",\"Rolling_Avg\",\"New_Case_Roll_Avg\",\n",
    "                                                  \"Change_New_Case_Avg\",\"Normalized_Change\"]].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trouble_flag == 1:\n",
    "    rolling_avg.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a dataframe that groups the counties for each site together\n",
    "site_avg = rolling_avg.groupby([\"Date\",\"Site\"])[[\"Confirmed\",\"Population\",\"New_Cases\",\"Rolling_Avg\",\"New_Case_Roll_Avg\",\n",
    "                                                 \"Change_New_Case_Avg\"]].sum().reset_index()\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    site_avg.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_avg[\"Conf_Per_100k\"] = site_avg.apply(lambda row: 100000*row.Confirmed/row.Population, axis = 1)\n",
    "site_avg[\"New_Per_100k\"]= site_avg.apply(lambda row: 100000*row.New_Cases/row.Population, axis = 1)\n",
    "site_avg[\"Conf_Avg_Per_100k\"] = site_avg.apply(lambda row: 100000*row.Rolling_Avg/row.Population, axis = 1)\n",
    "site_avg[\"New_Avg_Per_100k\"] = site_avg.apply(lambda row: 100000*row.New_Case_Roll_Avg/row.Population, axis = 1)\n",
    "\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    site_avg.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rolling_avg[\"Conf_Per_100k\"] = rolling_avg.apply(lambda row: 100000*row.Confirmed/row.Population, axis = 1)\n",
    "rolling_avg[\"New_Per_100k\"]= rolling_avg.apply(lambda row: 100000*row.New_Cases/row.Population, axis = 1)\n",
    "rolling_avg[\"Conf_Avg_Per_100k\"] = rolling_avg.apply(lambda row: 100000*row.Rolling_Avg/row.Population, axis = 1)\n",
    "rolling_avg[\"New_Avg_Per_100k\"] = rolling_avg.apply(lambda row: 100000*row.New_Case_Roll_Avg/row.Population, axis = 1)\n",
    "\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    rolling_avg.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define parameters for creating a separate dataframe with just current day's data\n",
    "today = datetime.datetime.today()\n",
    "offset2 = datetime.timedelta(days = 2)\n",
    "olddate = today - offset2\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    olddate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only most current day's data\n",
    "#rolling_avg[\"Date\"] = pd.to_datetime(rolling_avg.Date)\n",
    "rolling_avg_change = rolling_avg[rolling_avg[\"Date\"] > olddate]\n",
    "site_avg_change = site_avg[site_avg[\"Date\"] > olddate]\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    site_avg_change.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JBC4\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\JBC4\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "rolling_avg[\"Date\"] = rolling_avg[\"Date\"].dt.to_period(\"D\")\n",
    "rolling_avg_change[\"Date\"] = rolling_avg_change[\"Date\"].dt.to_period(\"D\")\n",
    "site_avg[\"Date\"] = site_avg[\"Date\"].dt.to_period(\"D\")\n",
    "site_avg_change[\"Date\"] = site_avg_change[\"Date\"].dt.to_period(\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_avg.to_excel(os.path.expanduser(\"~\") + '\\\\U.S. NRC\\\\COVID-19 Site Impacts - Documents\\\\site_status\\\\data\\\\simplified_site_data.xlsx')\n",
    "rolling_avg_change.to_excel(os.path.expanduser(\"~\") + '\\\\U.S. NRC\\\\COVID-19 Site Impacts - Documents\\\\site_status\\\\data\\\\site_today.xlsx')\n",
    "site_avg.to_excel(os.path.expanduser(\"~\") + '\\\\U.S. NRC\\\\COVID-19 Site Impacts - Documents\\\\site_status\\\\data\\\\by_site.xlsx')\n",
    "site_avg_change.to_excel(os.path.expanduser(\"~\") + '\\\\U.S. NRC\\\\COVID-19 Site Impacts - Documents\\\\site_status\\\\data\\\\by_site_today.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Status Update\n",
    "\n",
    "This portion of the script updates the power status data for the power status board. and consolidates the report by site.\n",
    "\n",
    "Data Source:  RPS (HOO report) because the data warehouse doesn't update power status until later in the day and people are generally interested in this data early in the morning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Today's Power Status Report\n",
    "\n",
    "# Clear  old downloaded .csv file if it exists:\n",
    "path_file = os.path.expanduser(\"~\")+'\\\\Downloads\\\\Power Reactor Status.csv'\n",
    "if os.path.exists(path_file):\n",
    "    os.unlink(path_file)\n",
    "\n",
    "# Get Current Power Status Report\n",
    "\n",
    "#Set chromedriver path and options:\n",
    "chrome_path = 'C:\\\\ProgramData\\\\Chromedriver\\\\chromedriver.exe';\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--disable-extensions')\n",
    "chrome_options.add_experimental_option('useAutomationExtension',False)\n",
    "browser = webdriver.Chrome(chrome_path,options=chrome_options)\n",
    "\n",
    "# Download the .csv power status report from RRPS:\n",
    "browser.get('https://hqvwdbrps04.nrc.gov/reportserver/pages/reportviewer.aspx?/Oversight/Reports/Power+Reactor+Status')\n",
    "time.sleep(4)\n",
    "browser.execute_script(\"$find('ReportViewerControl').exportReport('CSV');\")\n",
    "time.sleep(5)\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data files:\n",
    "power_data = pd.read_csv(path_file)\n",
    "file_path_site_hist = os.path.expanduser(\"~\") + '\\\\U.S. NRC\\\\COVID-19 Site Impacts - Documents\\\\site_covid_cumulative_site_history.csv'\n",
    "file_path_updates = os.path.expanduser(\"~\") + '\\\\U.S. NRC\\\\COVID-19 Site Impacts - Documents\\\\site_Impacts_for_updating.xlsx'\n",
    "site_hist_covid = pd.read_csv(file_path_site_hist)\n",
    "site_info_update = pd.read_excel(file_path_updates, sheet_name='SiteImpacts')\n",
    "site_info_fuelfac = pd.read_excel(file_path_updates, sheet_name='Fuel Facilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Append today's data to the site_cumulative_history file:\n",
    "date_today = datetime.date.today().strftime(\"%m/%d/%Y\")\n",
    "if date_today in site_hist_covid.columns:\n",
    "    print('Already there, not writing')\n",
    "else:\n",
    "    site_hist_covid[date_today]=site_info_update['Number of Confirmed Cases Onsite'].append(site_info_fuelfac['Number of Confirmed Cases Onsite'],ignore_index=True)\n",
    "    site_hist_covid.to_csv(file_path_site_hist,index=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format and export updated power data for today:\n",
    "power_data['Comments3'] = power_data['Comments3'].str.replace('\\r\\n','')\n",
    "power_data = power_data.fillna('')\n",
    "\n",
    "# combine unit information from power data file to site level:\n",
    "\n",
    "for ii in range(0,len(site_info_update)):\n",
    "    update_string = ''\n",
    "    down_count = 0\n",
    "    unit_count = 0\n",
    "    for jj in range(0,len(power_data)):\n",
    "            if site_info_update.iloc[ii,0] in power_data.iloc[jj,1]:\n",
    "                unit_count = unit_count+1\n",
    "                if power_data.iloc[jj,3] < 10:\n",
    "                    down_count = down_count+1\n",
    "                unit_num = re.findall(r'\\d+', power_data.iloc[jj,1])\n",
    "                if len(unit_num) == 0:\n",
    "                    if power_data.iloc[jj,3] != 100:\n",
    "                        update_string = update_string + str(power_data.iloc[jj,3]) + '% ' + power_data.iloc[jj,5] + ' ' + power_data.iloc[jj,4]\n",
    "                else:\n",
    "                    if power_data.iloc[jj,3] != 100:\n",
    "                        update_string = update_string + 'Unit ' + unit_num[0] + ': ' + str(power_data.iloc[jj,3]) + '% ' + power_data.iloc[jj,5] + ' ' + power_data.iloc[jj,4]\n",
    "\n",
    "    site_info_update.iloc[ii,13]=update_string\n",
    "    if 'Refueling' in site_info_update.iloc[ii,13]:\n",
    "        site_info_update.iloc[ii,15] = 'Y'\n",
    "    else:\n",
    "        site_info_update.iloc[ii,15] = 'N'\n",
    "    \n",
    "    if down_count == unit_count:\n",
    "        site_info_update.iloc[ii,14] = '0 - all units onsite shutdown'\n",
    "    elif down_count > 0:\n",
    "        site_info_update.iloc[ii,14] = '1 - one or more units shutdown'\n",
    "    else:\n",
    "        site_info_update.iloc[ii,14] = '2 - all units fully operational'\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Display results if in troubleshoot mode::\n",
    "if trouble_flag == 1:\n",
    "    site_info_update.iloc[:,[0,13,14,15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Export to file (was used to copy to site_impacts_for_updating since openpyxl resulted in errors)\n",
    "#  Now only needed for troubleshooting:\n",
    "\n",
    "if trouble_flag == 1:\n",
    "    site_info_update.to_excel('Site Impact Updates.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code calculates the reported history at the site from the cumulative history file:\n",
    "# Note:  Will likely read zero until the tracking process is resumed.\n",
    "# read in latest US confirmed and death time history data:\n",
    "\n",
    "covid_site_confirmed = site_hist_covid\n",
    "\n",
    "# drop all but the last two weeks of data:\n",
    "covid_site_confirmed.drop(covid_site_confirmed.iloc[:, 1:-15], inplace = True, axis = 1)\n",
    "\n",
    "\n",
    "# #### Calculate daily and weekly totals:\n",
    "# \n",
    "# The following code calculates the daily, weekly and biweekly totals for confirmed cases and deaths, since the JHU data always appends the current days date as the last column in their dataset.\n",
    "\n",
    "new_cases_site_daily = covid_site_confirmed[covid_site_confirmed.columns[-1]]-covid_site_confirmed[covid_site_confirmed.columns[-2]]\n",
    "new_cases_site_weekly = covid_site_confirmed[covid_site_confirmed.columns[-1]]-covid_site_confirmed[covid_site_confirmed.columns[-6]]\n",
    "new_cases_site_biweekly = covid_site_confirmed[covid_site_confirmed.columns[-1]]-covid_site_confirmed[covid_site_confirmed.columns[-11]]\n",
    "\n",
    "# Print totals as a check:\n",
    "if trouble_flag == 1:\n",
    "    print('Total new site cases in last 24 hours  :  ' + str(new_cases_site_daily.sum()))\n",
    "    print('Total new site cases in last week      :  ' + str(new_cases_site_weekly.sum()))\n",
    "    print('Total new site cases in last two weeks :  ' + str(new_cases_site_biweekly.sum()))\n",
    "\n",
    "#  Append the new data to the dataframes for confirmed cases and deaths:\n",
    "covid_site_confirmed['New Cases Daily']=new_cases_site_daily\n",
    "covid_site_confirmed['New Cases Weekly']=new_cases_site_weekly\n",
    "covid_site_confirmed['New Cases Biweekly']=new_cases_site_biweekly\n",
    "covid_site_confirmed['Average for Last Week']=covid_site_confirmed['New Cases Weekly'].div(7).round(0)\n",
    "covid_site_confirmed['Average Past 2 Weeks']=covid_site_confirmed['New Cases Biweekly'].div(14).round(0)\n",
    "\n",
    "output_df = covid_site_confirmed.iloc[:,[0,16,17,18,19,20]]\n",
    "# Write the new files to a .csv file that has the calculated fields as the last columns:\n",
    "file_path_site_stats = os.path.expanduser(\"~\") + '\\\\U.S. NRC\\\\COVID-19 Site Impacts - Documents\\\\site_status\\\\data\\\\current_confirmed_site_data.csv'\n",
    "output_df.to_csv(file_path_site_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings and Power Status Data Update\n",
    "\n",
    "This code pulls the DW source data for the findings search tool and updates the xlsx file on Sharepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "('HY000', '[HY000] [Microsoft][ODBC SQL Server Driver]Cannot generate SSPI context (0) (SQLDriverConnect); [HY000] [Microsoft][ODBC SQL Server Driver]Cannot generate SSPI context (0)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-3b4ab5a6d30f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m conn = pyodbc.connect('Driver={SQL Server};' \n\u001b[0m\u001b[0;32m      2\u001b[0m                       \u001b[1;34m'Server=hqvwepmdb01;'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                       \u001b[1;34m'Database=NRC_DW;'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                       'Trusted_Connection=yes;') \n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mError\u001b[0m: ('HY000', '[HY000] [Microsoft][ODBC SQL Server Driver]Cannot generate SSPI context (0) (SQLDriverConnect); [HY000] [Microsoft][ODBC SQL Server Driver]Cannot generate SSPI context (0)')"
     ]
    }
   ],
   "source": [
    "conn = pyodbc.connect('Driver={SQL Server};' \n",
    "                      'Server=hqvwepmdb01;' \n",
    "                      'Database=NRC_DW;' \n",
    "                      'Trusted_Connection=yes;') \n",
    " \n",
    "cursor = conn.cursor() \n",
    " \n",
    "sql_query = pd.read_sql_query('SELECT * FROM NRC_DW.dbo.vw_RPS_Inspections',conn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findings_file_path = os.path.expanduser(\"~\") + '\\\\U.S. NRC\\\\ROPDashboards - Documents\\\\Databases\\\\findings_search\\\\vw_RPS_Inspections.xlsx'\n",
    "power_file_path = os.path.expanduser(\"~\") + '\\\\U.S. NRC\\\\ROPDashboards - Documents\\\\Databases\\\\power_status_report\\\\power_status_data.xlsx'\n",
    "current_power_file = os.path.expanduser(\"~\") + '\\\\U.S. NRC\\\\ROPDashboards - Documents\\\\Databases\\\\power_status_report\\\\power_status_today.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query.to_excel(findings_file_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pyodbc.connect('Driver={SQL Server};' \n",
    "                      'Server=hqvwepmdb01;' \n",
    "                      'Database=NRC_Master;' \n",
    "                      'Trusted_Connection=yes;') \n",
    " \n",
    "cursor = conn.cursor() \n",
    " \n",
    "sql_query = pd.read_sql_query('SELECT * FROM NRC_Master.dbo.rawRPS_ROE_HOOPowerReactorStatus',conn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter power history to only the past two years:\n",
    "d = datetime.datetime.today() - datetime.timedelta(days=730)\n",
    "sql_query = sql_query[(sql_query['ReportDate'] > d.strftime('%Y-%m-%d'))]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to file:\n",
    "\n",
    "sql_query.to_excel(power_file_path,index=False)\n",
    "\n",
    "# write today's information to file\n",
    "\n",
    "power_data.to_excel(current_power_file,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the current power status file to the Power Status directory with today's date as a filename:\n",
    "\n",
    "report_date_daily = datetime.date.today().strftime(\"%m-%d-%Y\")\n",
    "source_report_today = os.path.expanduser(\"~\")+'\\\\Downloads\\\\Power Reactor Status.csv'\n",
    "os.rename(source_report_today,os.path.expanduser(\"~\")+'\\\\Downloads\\\\' + report_date_daily + '.csv')\n",
    "dest_report_dir = os.path.expanduser(\"~\") + '\\\\U.S. NRC\\\\ROPDashboards - Documents\\\\Databases\\\\power_status_report\\\\daily\\\\'\n",
    "shutil.copy(os.path.expanduser(\"~\")+'\\\\Downloads\\\\' + report_date_daily + '.csv', dest_report_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Event Notification Data:\n",
    "event_notification_file_path = os.path.expanduser(\"~\") + '\\\\U.S. NRC\\\\ROPDashboards - Documents\\\\Databases\\\\OpE\\\\event_notification_raw_data.xlsx'\n",
    "sql_query = pd.read_sql_query('SELECT * FROM NRC_Master.dbo.rawRPS_ROE_HOOEventNotification',conn)\n",
    "sql_query.to_excel(event_notification_file_path,index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trouble_flag == 1:\n",
    "    sql_query.Text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update COVID board information from CDC and COVID Act Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_act_now = pd.read_csv('https://api.covidactnow.org/v2/counties.csv?apiKey=84ae403fda674c3d8590a7410f805f47')\n",
    "covid_act_now.to_csv(os.path.expanduser(\"~\") + '\\\\U.S. NRC\\\\COVID-19 Site Impacts - Documents\\\\site_status\\\\data\\\\covid_act_now.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = requests.get('https://healthdata.gov/api/odata/v4/6hii-ae4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_text = a.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdc_date_today = datetime.date.today()\n",
    "cdc_week_ago = cdc_date_today-datetime.timedelta(days=7)\n",
    "start_date = cdc_week_ago.strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = [] \n",
    "for ii in range(1,7):\n",
    "        check_date = cdc_date_today-datetime.timedelta(days=ii)\n",
    "        search_string = check_date.strftime(\"%Y%m%d\") + '.xlsx'\n",
    "        if search_string in a_text:\n",
    "            last_date = search_string\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_id = a_text.find(search_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_file_id = a_text[loc_id+32:loc_id+68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdc_file_url = 'https://healthdata.gov/api/views/gqxm-d9w9/files/'+hash_file_id+'?download=true&filename=Community%20Profile%20Report%'+search_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "outfilename = os.path.expanduser(\"~\") + '\\\\U.S. NRC\\\\COVID-19 Site Impacts - Documents\\\\site_status\\\\data\\\\Community_Profile_Report.xlsx'\n",
    "urllib.request.urlretrieve(cdc_file_url, outfilename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update OpE Documents Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LER_url = 'https://adams.nrc.gov/wba/services/search/advanced/nrc?q=(mode:sections,sections:(filters:(public-library:!t),options:(within-folder:(enable:!f,insubfolder:!f,path:%27%27)),properties_search_all:!(!(DocumentType,starts,%27Licensee+Event+Report%27,%27%27),!(DocketNumber,starts,%2705000%27,%27%27),!(DocumentDate,range,(left:%2701/01/2021+12:00+AM%27,right:%2712/31/2021+11:59+PM%27),%27%27))))&qn=New&tab=advanced-search-pars&s=%24title&so=ASC'\n",
    "  \n",
    "# creating HTTP response object from given url\n",
    "resp = requests.get(LER_url)\n",
    "# saving the xml file\n",
    "with open('LERfeed.xml', 'wb') as f:\n",
    "    f.write(resp.content)\n",
    "\n",
    "      \n",
    "xml_data = open('LERfeed.xml', 'r').read()  # Read file\n",
    "root = ET.XML(xml_data)  # Parse XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LER_titles = []\n",
    "LER_MLs = []\n",
    "LER_date = []\n",
    "LER_doctype = []\n",
    "LER_link = []\n",
    "for child in root.iter():\n",
    "    if child.tag == 'AccessionNumber':\n",
    "        LER_MLs.append(child.text)\n",
    "        LER_link.append('https://www.nrc.gov/docs/'+ child.text[0:6] + '/' + child.text + '.pdf')\n",
    "    elif child.tag == 'DocumentTitle':\n",
    "        LER_titles.append(child.text)\n",
    "    elif child.tag == 'DocumentDate':\n",
    "        LER_date.append(child.text)\n",
    "    elif child.tag == 'DocumentType':\n",
    "        LER_doctype.append(child.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_columns = ['Title', 'AccessionNumber','DocumentDate','DocumentType','Link']\n",
    "LER_data = pd.DataFrame(list(zip(LER_titles,LER_MLs,LER_date,LER_doctype,LER_link)),columns = document_columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LER_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ADAMS_pull(document_type, year):\n",
    "    doc_str = document_type.replace(' ','+')\n",
    "    API_url = 'https://adams.nrc.gov/wba/services/search/advanced/nrc?q=(mode:sections,sections:(filters:(public-library:!t),options:(within-folder:(enable:!f,insubfolder:!f,path:%27%27)),properties_search_all:!(!(DocumentType,starts,%27' + doc_str + '%27,%27%27),!(DocketNumber,starts,%2705000%27,%27%27),!(DocumentDate,range,(left:%2701/01/' + str(year) +'+12:00+AM%27,right:%2712/31/' + str(year) + '+11:59+PM%27),%27%27))))&qn=New&tab=advanced-search-pars&s=%24title&so=ASC'\n",
    "    # creating HTTP response object from given url\n",
    "    resp = requests.get(API_url)\n",
    "    \n",
    "    # saving the xml file\n",
    "    with open('document_feed.xml', 'wb') as f:\n",
    "        f.write(resp.content)\n",
    "\n",
    "    xml_data = open('document_feed.xml', 'r').read()  # Read file\n",
    "    root = ET.XML(xml_data)  # Parse XML\n",
    "    \n",
    "    doc_titles = []\n",
    "    doc_MLs = []\n",
    "    doc_date = []\n",
    "    doc_doctype = []\n",
    "    doc_link = []\n",
    "    for child in root.iter():\n",
    "        if child.tag == 'AccessionNumber':\n",
    "            doc_MLs.append(child.text)\n",
    "            doc_link.append('https://www.nrc.gov/docs/'+ child.text[0:6] + '/' + child.text + '.pdf')\n",
    "        elif child.tag == 'DocumentTitle':\n",
    "            doc_titles.append(child.text)\n",
    "        elif child.tag == 'DocumentDate':\n",
    "            doc_date.append(child.text)\n",
    "        elif child.tag == 'DocumentType':\n",
    "            doc_doctype.append(child.text)\n",
    "    \n",
    "    document_columns = ['Title', 'AccessionNumber','DocumentDate','DocumentType','Link']\n",
    "    doc_data = pd.DataFrame(list(zip(doc_titles,doc_MLs,doc_date,doc_doctype,doc_link)),columns = document_columns )\n",
    "    \n",
    "    return doc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LER_2020 = ADAMS_pull('Deficiency',2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LER_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
